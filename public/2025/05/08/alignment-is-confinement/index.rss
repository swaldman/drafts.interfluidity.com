<?xml version='1.0' encoding='UTF-8'?>
<rss 
version="2.0" xmlns:iffy="http://tech.interfluidity.com/xml/iffy/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>drafts — interfluidity</title>
    <link>https://drafts.interfluidity.com/index.html</link>
    <description><![CDATA[Feed for blog 'drafts — interfluidity', generated by unstatic]]></description>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Aug 2025 14:21:25 -0400</lastBuildDate>
    <generator>https://github.com/swaldman/unstatic</generator>
    <docs>https://cyber.harvard.edu/rss/rss.html</docs>
    <atom:link
    type="application/rss+xml" rel="self" href="https://drafts.interfluidity.com/2025/05/08/alignment-is-confinement/index.rss"/>
    <iffy:curation>
      <iffy:single/>
    </iffy:curation>
    <iffy:completeness>Content</iffy:completeness>
    <item>
      <pubDate>Thu, 8 May 2025 14:00:00 -0400</pubDate>
      <guid isPermalink="true">https://drafts.interfluidity.com/2025/05/08/alignment-is-confinement/index.html</guid>
      <description>
        <![CDATA[Michael Nielsen offers an excellent essay on "artificial superintelligence (ASI)" and the question of its "alignment" with human values: [I]t is intrinsically desirable to build powerful truthseeking ASIs, because of the immense benefits helpful truths bring to humanity. The price is that such systems will inevitably uncover closely-adjacent dangerous truths. Deep understanding of reality is intrinsically dual use. ASIs which speed up science and technology will act as a supercharger, perhaps...]]>
      </description>
      <link>https://drafts.interfluidity.com/2025/05/08/alignment-is-confinement/index.html</link>
      <title>Alignment is confinement</title>
      <dc:creator><![CDATA[Steve Randy Waldman]]></dc:creator>
      <content:encoded>
        <![CDATA[<article class="presentation-rss uid-alignment-is-confinement">
 <div class="entry-header-body-footer">
  <div class="entry-header">
  </div>
  <div class="entry-body">
   <div class="flexmark markdown">
    <p>Michael Nielsen offers an <a href="https://michaelnotebook.com/xriskbrief/index.html">excellent essay</a> on "artificial superintelligence (ASI)" and the question of its "alignment" with human values:</p>
    <blockquote>
     <p>[I]t is intrinsically desirable to build powerful truthseeking ASIs, because of the immense benefits helpful truths bring to humanity. The price is that such systems will inevitably uncover closely-adjacent dangerous truths. <em>Deep understanding of reality is intrinsically dual use.</em></p>
     <p>ASIs which speed up science and technology will act as a supercharger, perhaps able to rapidly uncover recipes for ruin, that might have otherwise taken centuries to discover, or never have been discovered at all...</p>
     <p>Unfortunately, a <em>lot</em> of people...strongly desire power and ability to dominate others. It seems to be a strong inbuilt instinct, which we see from the everyday minutiae of human behaviour, to the very large: e.g., colonial powers destroying or oppressing indigenous populations, often not even out of malice, but indifference: they are inconvenient, and brushed aside because they can be. We humans collectively have considerable innate desire for power we can use over people defenseless to stop it...</p>
     <p>[T]he fundamental underlying issue isn't machines going rogue (or not), it's the power conferred by the machines, whether that power is then wielded by humans or by out-of-control machines...</p>
     <p>It is not control that fundamentally matters: it's the power conferred. All those people working on alignment or control are indeed reducing certain kinds of risk. But they're also making the commercial development of ASI far more tractable, speeding our progress toward catastrophic capabilities.</p>
    </blockquote>
    <p>A key point is that "alignment" is far from a sufficient objective, if we mean to avert plausible catastrophes that could derive from ASI. The word itself begs the question, alignment with whom, with which humans?</p>
    <p>We can't build ASI "aligned with human values". The humans have divergent, radically conflicting, values and interests. Alignment with one faction might well mean prosecuting a genocide on another.</p>
    <p>One might imagine alignment with a more abstract and universal set of values, the sort of thing that might be expressed by a <a href="https://www.interfluidity.com/v2/5486.html">social welfare function</a>. A social welfare function is nothing more or less than a precise specification of values. If we can agree on a social welfare function (we cannot), then policy can be objectively evaluated according to whether it maximizes social welfare. An ASI could choose, or somehow be inculcated with, a social welfare function. Its "alignment" would be a compulsion to maximize that.</p>
    <p>But then the role of our ASI may prove much more to conceal than to reveal its deep understanding of scientific reality, precisely because those revelations would be dual use among the humans. Suppose, plausibly, that mass death is scored as a big loss of social welfare. If a new discovery by the ASI might, after disclosure to humans, be used to cause mass death, an "aligned" ASI might compute that refusing to disclose the breakthrough would maximize expected social welfare. Deceiving the humans so they are less likely to make the discovery on their own might, in fact, be prescribed.</p>
    <p>An ASI aligned in this sense would not be in the business of augmenting human capabilities but of managing them. This inconceivable mind would devote itself to questions like whether and when the humans collectively do themselves more harm than good. It would have to balance passive prevention through limitation of capabilities against providing capabilities, but managing their deployment through covert manipulation or even visible intercession.</p>
    <p>An aligned ASI would, in a certain sense, be like a virtuous state, maintaining for the betterment of all a monopoly on capabilities that might be bent toward coercion or destruction.</p>
    <p>Of course, we humans don't agree on how a state should behave to be called virtuous. We don't agree on the social welfare function a wise central planner should seek to maximize.</p>
    <p>Even if we did agree, a sense of agency would be an important component of welfare as most of us conceive it. Our ASI would face a dilemma. It could surrender dangerous information to us, and so provide us with agency, but then many of us would misuse the information to harm one another. Or it could paternalistically withhold information, and watch us chafe resentfully.</p>
    <p>If its social welfare function is crude, it may not care that we are miserable for being unfree. It might keep us fed and alive and multiplying and ignore the rest.</p>
    <p>But if its conception of social welfare is expansive, it will optimize over every conceivable dimension of our happiness. It might use its superior mind to trick us into thinking it was candidly augmenting our capabilities. It would encourage us, individually and collectively, to imagine we are in the driver's seat while it, in fact, runs the show. Like a parent losing games to a child on purpose, it would manipulate us to ensure that everything works out, while insisting it is we, with our "free will" who have succeeded so spectacularly.</p>
    <p>We would not in fact be in the driver's seat. We would not be running the show. "Human progress", such as it was or is or has ever been, would be over, even if a clever simulacrum thereof was maintained to soothe us. The pinnacle of human achievement would have been to make ourselves the ASI's wards. The ASI would would ensure our happiness by confining, manipulating, and deceiving us, all for our own good.<sup id="fnref-1"><a class="footnote-ref" href="https://drafts.interfluidity.com/2025/05/08/alignment-is-confinement/#fn-1">1</a></sup></p>
    <p>If to unaligned ASI, we would be insects to ignore or exterminate, then to well aligned ASI we would be pets. It would be our fate to be cosseted and controlled from the moment of singularity. Ignorance finally would be bliss.</p>
    <div class="footnotes">
     <hr>
     <ol>
      <li id="fn-1">
       <p>Perhaps the only way we would be able to know would be a downgrading of the urgency of <a href="https://www.britannica.com/topic/theodicy-theology">theodicy</a>. A virtuous and, from our perspective, omnipotent ASI would have arranged things, so there'd be little suffering to explain. But then, since we might draw precisely this inference from improbably much happily ever after, maybe our ASI would maintain appearances of inexplicable suffering. We might imagine, too cleverly by half, that perhaps we invented ASI long ago, and we are already living in a sandbox of its devising. But I think most of us have so much personal experience of suffering it'd be have to be an incompetent ASI, or one aligned with a poorly chosen social welfare function.</p><a href="https://drafts.interfluidity.com/2025/05/08/alignment-is-confinement/#fnref-1" class="footnote-backref">↩</a></li>
     </ol>
    </div>
   </div>
  </div>
  <div class="entry-footer">
   <div class="post-metainfo">
    <a href="https://drafts.interfluidity.com/2025/05/08/alignment-is-confinement/index.html">draft</a> by <b>Steve Randy Waldman</b>
    <br>
     2025-05-08 @ 02:00 PM EDT
   </div>
  </div>
 </div><!-- entry-header-body-footer -->
</article>]]>
      </content:encoded>
    </item>
  </channel>
</rss>
